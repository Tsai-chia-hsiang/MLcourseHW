{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Copy from https://www.kaggle.com/code/mayukh18/pytorch-fog-end-to-end-baseline-lb-0-254","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport time\n\nimport json\nfrom tqdm import tqdm\nimport glob\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom sklearn.model_selection import train_test_split, StratifiedGroupKFold\nfrom sklearn.metrics import accuracy_score, average_precision_score\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\nprint(\"Env set ok ..\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-07T18:12:42.078198Z","iopub.execute_input":"2023-06-07T18:12:42.078744Z","iopub.status.idle":"2023-06-07T18:12:42.087186Z","shell.execute_reply.started":"2023-06-07T18:12:42.078706Z","shell.execute_reply":"2023-06-07T18:12:42.086081Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Env set ok ..\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Stratified Group K Fold\n\nIt's mentioned in the data that the subjects are different in the train and test set and even different between the public/private splits of the test data. So we need to use Stratified Group K Fold. But since the positive instances in the sequences are very scarce, we need to pick up the best fold which will give us the best balance of the positive/negative instances.","metadata":{}},{"cell_type":"markdown","source":"### tdcsfog preprocessing","metadata":{}},{"cell_type":"code","source":"# Analysis of positive instances in each fold of our CV folds\n\nn1_sum = []\nn2_sum = []\nn3_sum = []\ncount = []\n\n# Here I am using the metadata file available during training. Since the code \n# will run again during submission, if \n# I used the usual file from the competition folder, \n# it would have been updated with the test files too.\nmetadata = pd.read_csv(\"/kaggle/input/copy-train-metadata/tdcsfog_metadata.csv\")\n\nfor f in tqdm(metadata['Id']):\n    fpath = f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{f}.csv\"\n    df = pd.read_csv(fpath)\n    \n    n1_sum.append(np.sum(df['StartHesitation']))\n    n2_sum.append(np.sum(df['Turn']))\n    n3_sum.append(np.sum(df['Walking']))\n    count.append(len(df))\n    \nprint(f\"32 files have positive values in all 3 classes\")\n\nmetadata['n1_sum'] = n1_sum\nmetadata['n2_sum'] = n2_sum\nmetadata['n3_sum'] = n3_sum\nmetadata['count'] = count\n\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    \n    print(f\"Length of Train = {len(train_index)}, Length of Valid = {len(valid_index)}\")\n    n1_sum = metadata.loc[train_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[train_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[train_index, 'n3_sum'].sum()\n    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n    n1_sum = metadata.loc[valid_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[valid_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[valid_index, 'n3_sum'].sum()\n    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n# FOLD 2 is the most well balanced\n# The actual train-test split (based on Fold 2)\n\nmetadata = pd.read_csv(\"/kaggle/input/copy-train-metadata/tdcsfog_metadata.csv\")\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    if i != 2:\n        continue\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n    \n    if i == 2:\n        break\n        \ntrain_fpaths_tdcs = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{_id}.csv\" for _id in train_ids]\nvalid_fpaths_tdcs = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/{_id}.csv\" for _id in valid_ids]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:13:16.990177Z","iopub.execute_input":"2023-06-07T18:13:16.991287Z","iopub.status.idle":"2023-06-07T18:13:32.958593Z","shell.execute_reply.started":"2023-06-07T18:13:16.991229Z","shell.execute_reply":"2023-06-07T18:13:32.957443Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"100%|██████████| 833/833 [00:15<00:00, 52.60it/s]","output_type":"stream"},{"name":"stdout","text":"32 files have positive values in all 3 classes\nFold = 0\nLength of Train = 672, Length of Valid = 161\nTrain classes: 287,832, 1,462,652, 175,633\nValid classes: 16,958, 216,130, 32,205\nFold = 1\nLength of Train = 613, Length of Valid = 220\nTrain classes: 51,748, 909,505, 65,242\nValid classes: 253,042, 769,277, 142,596\nFold = 2\nLength of Train = 703, Length of Valid = 130\nTrain classes: 271,881, 1,332,746, 183,673\nValid classes: 32,909, 346,036, 24,165\nFold = 3\nLength of Train = 649, Length of Valid = 184\nTrain classes: 303,710, 1,517,147, 205,196\nValid classes: 1,080, 161,635, 2,642\nFold = 4\nLength of Train = 695, Length of Valid = 138\nTrain classes: 303,989, 1,493,078, 201,608\nValid classes: 801, 185,704, 6,230\nFold = 2\nLength of Train = 703, Length of Valid = 130\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### defog preprocessing","metadata":{}},{"cell_type":"code","source":"# Analysis of positive instances in each fold of our CV folds\n\nn1_sum = []\nn2_sum = []\nn3_sum = []\ncount = []\n\n# Here I am using the metadata file available during training. Since the code will run again during submission, if \n# I used the usual file from the competition folder, it would have been updated with the test files too.\nmetadata = pd.read_csv(\"/kaggle/input/copy-train-metadata/defog_metadata.csv\")\nmetadata['n1_sum'] = 0\nmetadata['n2_sum'] = 0\nmetadata['n3_sum'] = 0\nmetadata['count'] = 0\n\nfor f in tqdm(metadata['Id']):\n    fpath = f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{f}.csv\"\n    if os.path.exists(fpath) == False:\n        continue\n        \n    df = pd.read_csv(fpath)\n    metadata.loc[metadata['Id'] == f, 'n1_sum'] = np.sum(df['StartHesitation'])\n    metadata.loc[metadata['Id'] == f, 'n2_sum'] = np.sum(df['Turn'])\n    metadata.loc[metadata['Id'] == f, 'n3_sum'] = np.sum(df['Walking'])\n    metadata.loc[metadata['Id'] == f, 'count'] = len(df)\n    \nmetadata = metadata[metadata['count'] > 0].reset_index()\n\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    \n    print(f\"Length of Train = {len(train_index)}, Length of Valid = {len(valid_index)}\")\n    n1_sum = metadata.loc[train_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[train_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[train_index, 'n3_sum'].sum()\n    print(f\"Train classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n    n1_sum = metadata.loc[valid_index, 'n1_sum'].sum()\n    n2_sum = metadata.loc[valid_index, 'n2_sum'].sum()\n    n3_sum = metadata.loc[valid_index, 'n3_sum'].sum()\n    print(f\"Valid classes: {n1_sum:,}, {n2_sum:,}, {n3_sum:,}\")\n    \n# FOLD 2 is the most well balanced\n# The actual train-test split (based on Fold 2)\n\nsgkf = StratifiedGroupKFold(n_splits=5, random_state=42, shuffle=True)\nfor i, (train_index, valid_index) in enumerate(sgkf.split(X=metadata['Id'], y=[1]*len(metadata), groups=metadata['Subject'])):\n    if i != 1:\n        continue\n    print(f\"Fold = {i}\")\n    train_ids = metadata.loc[train_index, 'Id']\n    valid_ids = metadata.loc[valid_index, 'Id']\n    print(f\"Length of Train = {len(train_ids)}, Length of Valid = {len(valid_ids)}\")\n    \n    if i == 2:\n        break\n        \ntrain_fpaths_de = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{_id}.csv\" for _id in train_ids]\nvalid_fpaths_de = [f\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog/{_id}.csv\" for _id in valid_ids]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:13:41.739335Z","iopub.execute_input":"2023-06-07T18:13:41.739811Z","iopub.status.idle":"2023-06-07T18:14:04.318144Z","shell.execute_reply.started":"2023-06-07T18:13:41.739746Z","shell.execute_reply":"2023-06-07T18:14:04.316865Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"100%|██████████| 137/137 [00:22<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold = 0\nLength of Train = 75, Length of Valid = 16\nTrain classes: 500, 428,683, 37,609\nValid classes: 0, 158,803, 60,910\nFold = 1\nLength of Train = 65, Length of Valid = 26\nTrain classes: 216, 490,429, 84,955\nValid classes: 284, 97,057, 13,564\nFold = 2\nLength of Train = 76, Length of Valid = 15\nTrain classes: 410, 488,634, 87,986\nValid classes: 90, 98,852, 10,533\nFold = 3\nLength of Train = 70, Length of Valid = 21\nTrain classes: 435, 424,494, 88,800\nValid classes: 65, 162,992, 9,719\nFold = 4\nLength of Train = 78, Length of Valid = 13\nTrain classes: 439, 517,704, 94,726\nValid classes: 61, 69,782, 3,793\nFold = 1\nLength of Train = 65, Length of Valid = 26\n","output_type":"stream"}]},{"cell_type":"code","source":"train_fpaths = [(f, 'de') for f in train_fpaths_de] + [(f, 'tdcs') for f in train_fpaths_tdcs]\nvalid_fpaths = [(f, 'de') for f in valid_fpaths_de] + [(f, 'tdcs') for f in valid_fpaths_tdcs]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:14:07.377303Z","iopub.execute_input":"2023-06-07T18:14:07.378409Z","iopub.status.idle":"2023-06-07T18:14:07.385339Z","shell.execute_reply.started":"2023-06-07T18:14:07.378364Z","shell.execute_reply":"2023-06-07T18:14:07.383922Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader\n\nWe use a window comprised of past and future time Acc readings to form our dataset for a particular time instance. In case some portion of the window data is not available, we pad them with zeros.","metadata":{}},{"cell_type":"code","source":"class FOGDataset(Dataset):\n    def __init__(self, fpaths, scale=9.806, split=\"train\"):\n        super(FOGDataset, self).__init__()\n        tm = time.time()\n        self.split = split\n        self.scale = scale\n        \n        self.fpaths = fpaths\n        self.dfs = [self.read(f[0], f[1]) for f in fpaths]\n        self.f_ids = [os.path.basename(f[0])[:-4] for f in self.fpaths]\n        \n        self.end_indices = []\n        self.shapes = []\n        _length = 0\n        for df in self.dfs:\n            self.shapes.append(df.shape[0])\n            _length += df.shape[0]\n            self.end_indices.append(_length)\n        \n        self.dfs = np.concatenate(self.dfs, axis=0).astype(np.float16)\n        self.length = self.dfs.shape[0]\n        \n        shape1 = self.dfs.shape[1]\n        \n        self.dfs = np.concatenate([np.zeros((cfg.wx*cfg.window_past, shape1)), self.dfs, np.zeros((cfg.wx*cfg.window_future, shape1))], axis=0)\n        print(f\"Dataset initialized in {time.time() - tm} secs!\")\n        gc.collect()\n        \n    def read(self, f, _type):\n        df = pd.read_csv(f)\n        if self.split == \"test\":\n            return np.array(df)\n        \n        if _type ==\"tdcs\":\n            df['Valid'] = 1\n            df['Task'] = 1\n            df['tdcs'] = 1\n        else:\n            df['tdcs'] = 0\n        \n        return np.array(df)\n            \n    def __getitem__(self, index):\n        if self.split == \"train\":\n            row_idx = random.randint(0, self.length-1) + cfg.wx*cfg.window_past\n        elif self.split == \"test\":\n            for i,e in enumerate(self.end_indices):\n                if index >= e:\n                    continue\n                df_idx = i\n                break\n\n            row_idx_true = self.shapes[df_idx] - (self.end_indices[df_idx] - index)\n            _id = self.f_ids[df_idx] + \"_\" + str(row_idx_true)\n            row_idx = index + cfg.wx*cfg.window_past\n        else:\n            row_idx = index + cfg.wx*cfg.window_past\n            \n        #scale = 9.806 if self.dfs[row_idx, -1] == 1 else 1.0\n        x = self.dfs[row_idx - cfg.wx*cfg.window_past : row_idx + cfg.wx*cfg.window_future, 1:4]\n        x = x[::cfg.wx, :][::-1, :]\n        x = torch.tensor(x.astype('float'))#/scale\n        \n        t = self.dfs[row_idx, -3]*self.dfs[row_idx, -2]\n        \n        if self.split == \"test\":\n            return _id, x, t\n        \n        y = self.dfs[row_idx, 4:7].astype('float')\n        y = torch.tensor(y)\n        \n        return x, y, t\n    \n    def __len__(self):\n        # return self.length\n        if self.split == \"train\":\n            return 5_000_000\n        return self.length","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:14:12.941889Z","iopub.execute_input":"2023-06-07T18:14:12.942638Z","iopub.status.idle":"2023-06-07T18:14:12.965135Z","shell.execute_reply.started":"2023-06-07T18:14:12.942599Z","shell.execute_reply":"2023-06-07T18:14:12.963630Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:14:16.319500Z","iopub.execute_input":"2023-06-07T18:14:16.320487Z","iopub.status.idle":"2023-06-07T18:14:16.458898Z","shell.execute_reply.started":"2023-06-07T18:14:16.320428Z","shell.execute_reply":"2023-06-07T18:14:16.457311Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nclass ResidualBlock(nn.Module) :\n    \n    def __init__(self, in_channel, out_channel) -> None:\n        super(ResidualBlock, self).__init__()\n        self.seq =nn.Sequential(\n            nn.Conv1d(in_channel, out_channel,3,padding=1),\n            nn.BatchNorm1d(out_channel),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(out_channel, out_channel, 3,padding=1),\n            nn.BatchNorm1d(out_channel)\n        )\n        self.neck = nn.Sequential()\n        if in_channel != out_channel:\n            self.neck = nn.Sequential(\n                nn.Conv1d(in_channel, out_channel,3,padding=1),\n                nn.BatchNorm1d(out_channel)\n            )\n        \n    def forward(self, x):\n        return F.relu(self.seq(x) + self.neck(x))\n\nclass ResNet20(nn.Module):\n    \n    def __init__(self, feature_channel) -> None:\n        super(ResNet20, self).__init__()       \n        self.conv1 = nn.Sequential(\n            nn.Conv1d(feature_channel, 16, 3, padding=1),\n            nn.BatchNorm1d(16),\n            nn.ReLU(inplace=True)\n        )\n        self.ResBlocks = nn.Sequential(\n            self.__ResBlock(16, 16),\n            self.__ResBlock(16, 32),\n            self.__ResBlock(32, 64),\n        )\n         \n    \n    def __ResBlock(self,cin, cout):\n        clist = [(cin, cout), (cout, cout), (cout, cout)]\n        return nn.Sequential(\n            *list(ResidualBlock(ci, co) for ci, co in clist)\n        )\n    def forward(self, x)->torch.Tensor :\n        x1 = self.conv1(x)\n        x1 = self.ResBlocks(x1)\n        x1 = F.avg_pool1d(x1.permute(0,2,1), kernel_size=3)\n        return x1.reshape(x1.shape[0], -1)\n\n\nclass ResConvClassifier(nn.Module):\n    def __init__(\n            self, feature_channels, seqlen, out_features, \n            add_classifiers = 0, Linear_hidden=128\n        ) -> None:\n        \n        super(ResConvClassifier, self).__init__()\n        self.emd = ResNet20(feature_channel=feature_channels)\n        resnet_out = seqlen*21\n        if (Linear_hidden > resnet_out):\n            print(f\"Waring ! Resnet20 output are {resnet_out}\")\n            print(f\"and you give {Linear_hidden}, try to lift dimension ????? \")\n            \n        c=[self.__LinBlock(resnet_out, Linear_hidden)]\n        for i in range(add_classifers + 1):\n            if i == add_classifers:\n                c.append(nn.Linear(Linear_hidden, out_features))\n            else:\n                c.append(self.__LinBlock(Linear_hidden, Linear_hidden))\n                \n                \n        self.classifer = nn.Sequential(*c)\n\n    def __LinBlock(self, ind, outd)->nn.Sequential:\n        return nn.Sequential(\n            nn.Linear(ind, outd),\n            nn.BatchNorm1d(outd),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, x):\n        x1 = self.emd(x.permute(0,2,1))\n        x1 = self.classifer(x1)\n        return x1","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:47:42.954222Z","iopub.execute_input":"2023-06-07T18:47:42.954625Z","iopub.status.idle":"2023-06-07T18:47:43.068347Z","shell.execute_reply.started":"2023-06-07T18:47:42.954585Z","shell.execute_reply":"2023-06-07T18:47:43.067086Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from torch.cuda.amp import GradScaler\n\ndef train_one_epoch(model, loader, optimizer, criterion, cfg):\n    loss_sum = 0.\n    scaler = GradScaler()\n    \n    model.train()\n    for x,y,t in tqdm(loader):                               #從DataLoader中取得一個Batch size的資料\n        x = x.to(cfg.device).float()\n        y = y.to(cfg.device).float()\n        t = t.to(cfg.device).float()\n        \n        y_pred = model(x)                                    #將Input x丟入模型，得到y_pred\n        loss = criterion(y_pred, y)                          #計算預測結果y_pred與GT y的loss\n        loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n        \n        t_sum = torch.sum(t)\n        if t_sum > 0:\n            loss = torch.sum(loss)/t_sum\n        else:\n            loss = torch.sum(loss)*0.\n        \n        # loss.backward()\n        scaler.scale(loss).backward()                        #根據loss做反向傳播\n        # optimizer.step()\n        scaler.step(optimizer)                               #優化器優化模型參數\n        scaler.update()                                      \n        \n        optimizer.zero_grad()                                #優化器歸零\n        \n        loss_sum += loss.item()\n    total_loss = (loss_sum/len(loader))\n    print(f\"Train Loss: {total_loss:.04f}\")\n    \n    return total_loss\n\ndef validation_one_epoch(model, loader, criterion, cfg):\n    loss_sum = 0.\n    y_true_epoch = []\n    y_pred_epoch = []\n    t_valid_epoch = []\n    \n    model.eval()\n    for x,y,t in tqdm(loader):\n        x = x.to(cfg.device).float()\n        y = y.to(cfg.device).float()\n        t = t.to(cfg.device).float()\n        \n        with torch.no_grad():                                #沒有反向傳播\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n            loss = torch.mean(loss*t.unsqueeze(-1), dim=1)\n            \n            t_sum = torch.sum(t)\n            if t_sum > 0:\n                loss = torch.sum(loss)/t_sum\n            else:\n                loss = torch.sum(loss)*0.\n        \n        loss_sum += loss.item()\n        y_true_epoch.append(y.cpu().numpy())\n        y_pred_epoch.append(y_pred.cpu().numpy())\n        t_valid_epoch.append(t.cpu().numpy())\n        \n    y_true_epoch = np.concatenate(y_true_epoch, axis=0)\n    y_pred_epoch = np.concatenate(y_pred_epoch, axis=0)\n    \n    t_valid_epoch = np.concatenate(t_valid_epoch, axis=0)\n    y_true_epoch = y_true_epoch[t_valid_epoch > 0, :]\n    y_pred_epoch = y_pred_epoch[t_valid_epoch > 0, :]\n    \n    scores = [average_precision_score(y_true_epoch[:,i], y_pred_epoch[:,i]) for i in range(3)]\n    mean_score = np.mean(scores)\n    print(f\"Validation Loss: {(loss_sum/len(loader)):.04f}, Validation Score: {mean_score:.03f}, ClassWise: {scores[0]:.03f},{scores[1]:.03f},{scores[2]:.03f}\")\n    \n    return mean_score\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:16:14.547000Z","iopub.execute_input":"2023-06-07T18:16:14.547573Z","iopub.status.idle":"2023-06-07T18:16:14.570214Z","shell.execute_reply.started":"2023-06-07T18:16:14.547533Z","shell.execute_reply":"2023-06-07T18:16:14.568601Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Config:\n    train_dir1 = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/defog\"\n    train_dir2 = \"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog\"\n\n    batch_size = 1024                              #batch size大小\n    window_size = 32                               #每次Input的總時序資料長度\n    window_future = 8                              #取目標時間點後多少為Input\n    window_past = window_size - window_future      #取目標時間點前多少為Input\n    \n    wx = 8                                         #資料padding長度的參數\n    \n    optimizer_name = \"Adam\"                        #optimizer\n    loss_function = \"BCEWithLogitsLoss\"            #loss function\n    \n    \n    lr = 0.01                                   #learning rate\n    num_epochs =50                                #訓練的epochs數 8-10\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'         \n    \n    feature_list = ['AccV', 'AccML', 'AccAP']\n    label_list = ['StartHesitation', 'Turn', 'Walking']\n    \n    \ncfg = Config()\nprint(\"Config set ok\")","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:28:16.405215Z","iopub.execute_input":"2023-06-07T18:28:16.405698Z","iopub.status.idle":"2023-06-07T18:28:16.421576Z","shell.execute_reply.started":"2023-06-07T18:28:16.405656Z","shell.execute_reply":"2023-06-07T18:28:16.420503Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Config set ok\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = FOGDataset(train_fpaths, split=\"train\")\nvalid_dataset = FOGDataset(valid_fpaths, split=\"valid\")\nprint(f\"lengths of datasets: train - {len(train_dataset)}, valid - {len(valid_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:16:21.456764Z","iopub.execute_input":"2023-06-07T18:16:21.457246Z","iopub.status.idle":"2023-06-07T18:17:17.871923Z","shell.execute_reply.started":"2023-06-07T18:16:21.457204Z","shell.execute_reply":"2023-06-07T18:17:17.870517Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Dataset initialized in 43.15606451034546 secs!\nDataset initialized in 12.990145206451416 secs!\nlengths of datasets: train - 5000000, valid - 4984740\n","output_type":"stream"}]},{"cell_type":"code","source":"model = ResConvClassifier(\n    feature_channels=len(cfg.feature_list), \n    seqlen=cfg.window_size, \n    out_features=len(cfg.label_list)), \n    add_classifiers=2, Linear_hidden=256\n).to(cfg.device)\nprint(f\"Number of parameters in model - {count_parameters(model):,}\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=5, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=cfg.batch_size, num_workers=5)\n\noptimizer = getattr(torch.optim, cfg.optimizer_name)(model.parameters(), lr=cfg.lr)\ncriterion = getattr(torch.nn, cfg.loss_function)(reduction='none').to(cfg.device)\n#sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.85)\n\nloss_per_iter = []\nvalid_per_iter = []\nmax_score = 0.0\n\nprint(\"=\"*50)\nfor epoch in range(cfg.num_epochs):\n    print(f\"Epoch: {epoch}\")\n    loss = train_one_epoch(model, train_loader, optimizer, criterion,cfg)\n    score = validation_one_epoch(model, valid_loader, criterion, cfg)\n    #sched.step()\n    loss_per_iter.append(loss)\n    valid_per_iter.append(score)\n    \n    if score > max_score:\n        max_score = score\n        torch.save(model.state_dict(), \"best_model_state_b1.h5\")      #儲存最好的模型參數\n        print(\"Saving Model ...\")\n\n    print(\"=\"*50)\n    \ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recording(modelname):\n    metrices_root = os.path.join(\"metrics\")\n    if not os.path.exists(metrices_root):\n        os.mkdir(metrices_root)\n\n    with open(os.path.join(metrices_root, f\"{modelname}.json\"), 'w+') as j:\n        json.dump(\n            {'training loss':loss_per_iter,\"valid score\":valid_per_iter},\n            j,indent=4, ensure_ascii=False\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"model =ResConvClassifier(\n    feature_channels=3, seqlen=cfg.window_size, out_features=3, \n    add_classifiers=2, Linear_hidden=256\n).to(cfg.device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model_state_b1.h5\"))             #取得最好的模型參數\nmodel.eval()\n\ntest_defog_paths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/*.csv\")\ntest_tdcsfog_paths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/*.csv\")\ntest_fpaths = [(f, 'de') for f in test_defog_paths] + [(f, 'tdcs') for f in test_tdcsfog_paths]\n\ntest_dataset = FOGDataset(test_fpaths, split=\"test\")\ntest_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, num_workers=5)\n\nids = []\npreds = []\n\nfor _id, x, _ in tqdm(test_loader):\n    x = x.to(cfg.device).float()\n    with torch.no_grad():\n        y_pred = model(x)*0.1\n    \n    ids.extend(_id)\n    preds.extend(list(np.nan_to_num(y_pred.cpu().numpy())))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T18:58:36.258900Z","iopub.execute_input":"2023-06-07T18:58:36.259952Z","iopub.status.idle":"2023-06-07T18:58:45.068429Z","shell.execute_reply.started":"2023-06-07T18:58:36.259907Z","shell.execute_reply":"2023-06-07T18:58:45.066867Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Dataset initialized in 0.5198473930358887 secs!\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 83/280 [00:07<00:17, 11.01it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1474014428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/3888239019.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv\")\nsample_submission.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.array(preds)\nsubmission = pd.DataFrame(\n    {\n        'Id': ids, \n        'StartHesitation': np.round(preds[:,0],5),                 \n        'Turn': np.round(preds[:,1],5), \n        'Walking': np.round(preds[:,2],5)\n    }\n)\n\nsubmission = pd.merge(sample_submission[['Id']], submission, how='left', on='Id').fillna(0.0)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(submission.shape)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}