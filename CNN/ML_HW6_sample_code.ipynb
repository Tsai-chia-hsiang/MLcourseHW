{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0KSCsJ0y2XTY"},"source":["# ML Assignment 6 - Sample Code\n","* 雲端硬碟: https://drive.google.com/drive/folders/1KqXE_drqYYwg9RsQil3oXQeskXzuATdR?usp=sharing\n","* 蘭花競賽網站: https://tbrain.trendmicro.com.tw/Competitions/Details/20\n","\n","## 執行方式\n","資料集請去雲端硬碟取得(./dataset)，內有壓縮檔可下載\n","\n","依作業要求，在圖像轉換區塊更改程式碼。\n","訓練過程及輸出位於最後面。\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BOJJwMUtveYP"},"source":["## 初始設定"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5565,"status":"ok","timestamp":1684650683179,"user":{"displayName":"葉祺騰","userId":"04282943840272332804"},"user_tz":-480},"id":"uncYZYULmuqp","outputId":"2ffb80e4-52c6-464b-bb5b-bf46ef8c372a"},"outputs":[],"source":["import time\n","import os\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.hub import load_state_dict_from_url\n","import torchvision\n","from torchvision import models, transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import ConcatDataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import pandas as pd\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LsUYQ1BPvH7a"},"source":["## 圖像轉換\n","### 題目\n","torchvision.transforms 提供了許多可靠的 API來讓使用者對圖像進行操作，請試著在 data transforms 當中對訓練集進行轉換(圖像前處理)，當模型訓練到一定程度時，驗證看看使用該方法是否確實對模型準確率造成影響\n","\n","* **Weak Augmentation** - 使用**1**種data transforms，並記錄其**使用前、使用後的validation accuracy**，共做1~3次\n","\n","* **Strong Augmentation** - 使用**4~6**種data transforms，並記錄其**使用前、使用後的validation accuracy**\n","\n","### 下面列出目前全部可用的transforms，參數部分自行Google :)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORjQXKTVmyuA"},"outputs":[],"source":["class MyCNN(nn.Module):\n","\n","  def __init__(self, num_classes=1000):\n","    super(MyCNN, self).__init__()\n","    self.features = nn.Sequential(\n","      nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","      nn.ReLU(inplace=True),\n","      nn.MaxPool2d(kernel_size=3, stride=2),\n","      nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","      nn.ReLU(inplace=True),\n","      nn.MaxPool2d(kernel_size=3, stride=2),\n","      nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","      nn.ReLU(inplace=True),\n","      nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","      nn.ReLU(inplace=True),\n","      nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","      nn.ReLU(inplace=True),\n","      nn.MaxPool2d(kernel_size=3, stride=2),\n","    )\n","    self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","    self.classifier = nn.Sequential(\n","      nn.Dropout(),\n","      nn.Linear(256 * 6 * 6, 4096),\n","      nn.ReLU(inplace=True),\n","      nn.Dropout(),\n","      nn.Linear(4096, 4096),\n","      nn.ReLU(inplace=True),\n","    )\n","    self.classifier2 = nn.Sequential(\n","      nn.Linear(4096, num_classes),\n","    )\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = self.avgpool(x)\n","    x = torch.flatten(x, 1)\n","    x = self.classifier(x)\n","    x = self.classifier2(x)\n","\n","    return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lNezyEHCGbiq"},"source":["## 訓練模型區塊\n","包含視覺化模型及訓練模型。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualize_model(\n","    model, device, dataloaders, class_names, num_images=6,\n","    savepath=os.path.join(\"default\")\n","  ):\n","  if not os.path.exists(savepath):\n","    os.mkdir(savepath)\n","  \n","  was_training = model.training\n","  model.eval()\n","  images_so_far = 0\n","\n","  plt.figure(figsize=(18,9))\n","\n","  with torch.no_grad():\n","    for i, (inputs, labels) in enumerate(dataloaders['val']):\n","      inputs = inputs.to(device)\n","      labels = labels.to(device)\n","\n","      outputs = model(inputs)\n","      _, preds = torch.max(outputs, 1)\n","\n","      for j in range(inputs.size()[0]):\n","        images_so_far += 1\n","\n","        img_display = np.transpose(inputs.cpu().data[j].numpy(), (1,2,0)) #numpy:CHW, PIL:HWC\n","        plt.subplot(num_images//2,2,images_so_far),plt.imshow(img_display) #nrow,ncol,image_idx\n","        plt.title(f'predicted: {class_names[preds[j]]}')\n","        plt.savefig(os.path.join(savepath,\"test.jpg\"))\n","        if images_so_far == num_images:\n","            model.train(mode=was_training)\n","            plt.clf()\n","            return\n","    plt.clf()\n","    model.train(mode=was_training)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def imshow(inp, title=None):\n","  \"\"\"Imshow for Tensor.\"\"\"\n","  inp = inp.numpy().transpose((1, 2, 0))\n","  mean = np.array([0.485, 0.456, 0.406])\n","  std = np.array([0.229, 0.224, 0.225])\n","  \n","  #原先Normalize是對每個channel個別做 減去mean, 再除上std\n","  inp1 = std * inp + mean\n","\n","  plt.imshow(inp)\n","\n","  if title is not None:\n","      plt.title(title)\n","  plt.pause(0.001)  # pause a bit so that plots are updated\n","  plt.imshow(inp1)\n","  if title is not None:\n","      plt.title(title)\n","  plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkP2g__jm8M4"},"outputs":[],"source":["\n","def count_parameters(model):\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-dRfFb3q6Vm"},"outputs":[],"source":["def train_model(\n","    model, criterion, device, \n","    dataloaders, dataset_sizes, optimizer, scheduler, \n","    num_epochs=25\n","):\n","  since = time.time()\n","\n","  best_model_wts = copy.deepcopy(model.state_dict())\n","  best_acc = 0.0\n","  train_loss, valid_loss = [], []\n","  train_acc, valid_acc = [], []\n","\n","  for epoch in range(num_epochs):\n","    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","    print('-' * 10)\n","\n","    # Each epoch has a training and validation phase\n","    for phase in ['train', 'val']:\n","      if phase == 'train':\n","        model.train()  # Set model to training mode\n","      else:\n","        model.eval()   # Set model to evaluate mode\n","\n","      running_loss = 0.0\n","      running_corrects = 0\n","\n","      # Iterate over data.\n","      for inputs, labels in tqdm(dataloaders[phase]):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # forward\n","        # track history if only in train\n","        with torch.set_grad_enabled(phase == 'train'):\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","            # backward + optimize only if in training phase\n","            if phase == 'train':\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","        # statistics\n","        running_loss += loss.item() * inputs.size(0)\n","        running_corrects += torch.sum(preds == labels.data)\n","      if phase == 'train':\n","        scheduler.step()\n","\n","      epoch_loss = running_loss / dataset_sizes[phase]\n","      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","      if phase == 'train':\n","        train_loss.append(epoch_loss)\n","        train_acc.append(epoch_acc.cpu().item())\n","      else:\n","        valid_loss.append(epoch_loss)\n","        valid_acc.append(epoch_acc.cpu().item())\n","\n","      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","        phase, epoch_loss, epoch_acc))\n","\n","      # deep copy the model\n","      if phase == 'val' and epoch_acc > best_acc:\n","        best_acc = epoch_acc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","\n","\n","  time_elapsed = time.time() - since\n","  print('Training complete in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))\n","  print('Best val Acc: {:4f}'.format(best_acc))\n","\n","  # load best model weights\n","  model.load_state_dict(best_model_wts)\n","  #torch.save(model.state_dict(),\"model.pt\")\n","  return model, {\n","    'train loss':train_loss,\n","    'train acc':train_acc,\n","    'val loss':valid_loss,\n","    'val acc':valid_acc\n","  }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_hist_cmp(train_loss:dict, valid_loss:dict, root=os.path.join(\"cmp\")):\n","    \n","    if not os.path.exists(root):\n","        os.mkdir(root)\n","    \n","    plt.figure(dpi=800)\n","    for name, tloss in train_loss.items():\n","        plt.plot(range(1,len(tloss)+1,1), np.array(tloss), label= name) \n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.grid()\n","    plt.legend()\n","    plt.savefig(os.path.join(root,\"cmp_train_loss.jpg\"))\n","    plt.close()\n","\n","    plt.figure(dpi=800)\n","    for name, vloss in valid_loss.items():\n","        plt.plot(range(1,len(vloss)+1,1), np.array(vloss), label= name) #--evaluate_during_training True 在啟用eval\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.grid()\n","    plt.legend()\n","    plt.savefig(os.path.join(root, \"cmp_eval_loss.jpg\"))\n","    plt.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4To2Db6HG8vI"},"source":["## 主函式\n","- hyp:\n","    - num_epochs: 訓練回合數\n","    - lr: 訓練速度(learning rate)\n","    - batch_size: 批次(batch)大小\n","- preprocess:\n","    圖片要放進 model 前要做的處理 \n","    - (不用放 ```transforms.ToTensor()```)\n","- argumentation:\n","    argumentation methods:\n","    - a 2D list, each one contains 1 argumentation method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main(data_dir :os.PathLike, preprocess:list, \n","    argumentation:list = None, \n","    hyp = {'num_epochs':20, 'lr':0.001,'batchsize': 64},\n","    vismodel_savepath = os.path.join(\"default\") ):\n","\n","    num_workers = 2\n","    momentum = 0.9\n","    \n","    train_data_dir = os.path.join(data_dir, \"train\")\n","    valid_data_dir = os.path.join(data_dir, \"val\")\n","    \n","    to_tensor = [transforms.ToTensor()] \n","    default_preprocess = transforms.Compose(preprocess + to_tensor)\n","    train_prepreocess = ImageFolder(train_data_dir, default_preprocess)\n","    class_names = train_prepreocess.classes\n","    train_img_datasets = [train_prepreocess]\n","    bsize = hyp[\"batchsize\"]\n","    if argumentation is not None:\n","        for arg_method in argumentation:\n","           \n","            t = arg_method + preprocess + to_tensor\n","            print(t)\n","            argumentation_trans = transforms.Compose(t)\n","            train_img_datasets.append(\n","                ImageFolder(train_data_dir,argumentation_trans)\n","            )\n","    train_img_datasets = ConcatDataset(train_img_datasets)\n","    \n","    valid_img_datasets = ImageFolder(\n","        valid_data_dir,default_preprocess\n","    )\n","    \n","    dataloaders = {\n","        'train': DataLoader(\n","            train_img_datasets, batch_size=bsize,\n","            shuffle=True, num_workers=num_workers\n","        ) ,\n","        'val': DataLoader(\n","            valid_img_datasets,batch_size=bsize,\n","            shuffle=True, num_workers=num_workers\n","        )\n","    } \n","    dataset_sizes = {\n","        'train': len(train_img_datasets),\n","        'val': len(valid_img_datasets)\n","    }\n","    print(dataset_sizes)\n","    \n","  \n","    device = torch.device(\n","        \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","    )\n","    print(f\"Using device {device}\\n\")\n","\n","    # Get a batch of training data\n","    inputs, classes = next(iter(dataloaders['train']))\n","    # Make a grid from batch\n","    out = torchvision.utils.make_grid(inputs)\n","    \n","    \n","    imshow(out, title=[class_names[x] for x in classes])\n","\n","\n","    model_ft = MyCNN(num_classes=219)\n","    pretrained_dict = load_state_dict_from_url(\n","        'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n","        progress=True\n","    )\n","    model_dict = model_ft.state_dict()\n","    # 1. filter out unnecessary keys\n","    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n","    # 2. overwrite entries in the existing state dict\n","    model_dict.update(pretrained_dict) \n","    # 3. load the new state dict\n","    model_ft.load_state_dict(model_dict)\n","\n","    #for k,v in model_dict.items():\n","    #  print(k)\n","\n","    model_ft = model_ft.to(device)\n","\n","\n","    parameter_count = count_parameters(model_ft)\n","    print(f\"#parameters:{parameter_count}\")\n","    print(f\"batch_size:{bsize}\")\n","\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer_ft = optim.SGD(\n","        model_ft.parameters(), lr=hyp['lr'], momentum=momentum\n","    )\n","    exp_lr_scheduler = lr_scheduler.StepLR(\n","        optimizer_ft, step_size=7, gamma=0.1\n","    )\n","    model_ft, history = train_model(\n","        model_ft, criterion, device, \n","        dataloaders, dataset_sizes, optimizer_ft, \n","        exp_lr_scheduler, num_epochs=hyp['num_epochs']\n","    )\n","    class_names = valid_img_datasets.classes\n","    visualize_model(\n","        model_ft, device, \n","        dataloaders, class_names, \n","        savepath=vismodel_savepath\n","    )\n","    return history\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hyp = {\n","        'num_epochs':20, 'lr':0.001,\n","        'batchsize': 64\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_preprocessing = [\n","    transforms.Resize((224,224) )\n","]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Defualt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_metrics = main(\n","    data_dir=os.path.join(\"training\"),\n","    preprocess=data_preprocessing, hyp=hyp,\n","    vismodel_savepath=os.path.join(\"default\")\n",")\n","\n","torch.cuda.empty_cache()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Weak argumentation\n","\n","with 1 method => len( dataset ) = 2*len(origin dataset ) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["argumentation =[ \n","    [\n","        transforms.GaussianBlur(kernel_size=21, sigma=5)\n","    ]\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["guassian_metrics = main(\n","    data_dir=os.path.join(\"training\"),\n","    preprocess=data_preprocessing,\n","    hyp=hyp,\n","    argumentation=argumentation,\n","    vismodel_savepath=os.path.join(\"guassianblur\")\n",")\n","\n","torch.cuda.empty_cache()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Strong argumentation\n","\n","with 1 method => len( dataset ) = 2*len(origin dataset ) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#strong \n","s_argumentation = [\n","    [\n","        transforms.GaussianBlur(kernel_size=21, sigma=5),\n","        transforms.RandomAffine(degrees=(-30,30)),\n","        transforms.ColorJitter(brightness=(0, 5)),\n","        transforms.CenterCrop(size=300)\n","    \n","    ]\n","]\n","\n","strong_arg_metrics =  main(\n","    data_dir=os.path.join(\"training\"),\n","    preprocess=data_preprocessing,\n","    hyp=hyp,\n","    argumentation=s_argumentation,\n","    vismodel_savepath=os.path.join(\"strong\")\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## CMP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trains = {\n","    'default':default_metrics['train loss'],\n","    '+gussianblur':guassian_metrics['train loss'],\n","    'strong':strong_arg_metrics['train loss']\n","}\n","\n","valids = {\n","    'default':default_metrics['val loss'],\n","    '+gussianblur' : guassian_metrics['val loss'],\n","    'strong':strong_arg_metrics['val loss']\n","}\n","r = os.path.join(\"cmp\")\n","if not os.path.exists(r):\n","    os.mkdir(r)\n","\n","plot_hist_cmp(\n","    train_loss=trains, valid_loss=valids,root=r\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_val_acc = pd.DataFrame(\n","    {\n","        'default':[max(default_metrics['val acc'])],\n","        '+gussianblur':[max(guassian_metrics['val acc'])],\n","        'strong':[max(strong_arg_metrics['val acc'])]\n","    }\n",")\n","best_val_acc.to_csv(\n","    os.path.join(r,\"best_val_acc.csv\"), \n","    index=False\n",")\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1AfjALAXxlmqrxSs5M2ttBe1kont-SDjd","timestamp":1685440984630},{"file_id":"1sYEPYPmYsT1LfDrW0-XYTa9LqfeSIS9D","timestamp":1684141526082},{"file_id":"1Ujjhdn3UJYufPwV9-gVGQYYfkyINUudL","timestamp":1652349645157},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/62840b1eece760d5e42593187847261f/transfer_learning_tutorial.ipynb","timestamp":1643786436092}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
